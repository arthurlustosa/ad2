---
title: "Lab2 - Regressão Linear"
autor: "Arthur Lustosa"
output: 
  html_document:
    toc : true
    toc_float: true
    
---

```{r setup, include=FALSE}
library(dplyr)
library(reshape2)
library(ggplot2)
library(corrplot)
library(caret)
```


## Lendo os dados

```{r}
graduados.train <- read.csv("dados/graduados_treino.csv")
graduados.test <- read.csv("dados/graduados_teste.csv")
```

```{r}
colnames(graduados.train) <- c("matricula", "ano_evasao", "periodo_evasao", "cod_disciplina", "disciplina", "creditos", "media")
colnames(graduados.test) <- c("matricula", "ano_evasao", "periodo_evasao", "cod_disciplina", "disciplina", "creditos", "media")
```

## Conhecendo os dados
Os dados usados são referentes ao histórico de alunos do curso de computação da UFCG. A tarefa é, utilizando regressão linear, explicar o desempenho acadêmico.

O dataset inicial possui 15751 observações com 7 variáveis. No processo de leitura algumas alterações tiveram de ser feitas, encontramos muitos NaNs e tivemos que removelos para não influenciar nas análises que serão feitas. Depois calculamos os CRAs (Coeficiente de Rendimento Acadêmico) dos alunos. 
```{r warning=F, message=F}

# DADOS TREINO

#ordenando os dados pela matrícula
graduados.train <- graduados.train %>%
  arrange(matricula)

#filtrando dados e removendo os NaNs
graduados.clean <- graduados.train %>%
  filter(!is.na(media))

#calculando CRA dos alunos e salvando numa coluna
graduados.cra <- graduados.clean %>%
  group_by(matricula) %>%
  mutate(cra.contrb = media*creditos) %>%
  summarise(cra = sum(cra.contrb)/sum(creditos))

#utilizando a função dcast para deixar o dataset na forma ideal para a análise
graduados.model.input <- graduados.clean %>%
  group_by(matricula, disciplina) %>%
  filter(media == max(media))%>%
  ungroup() %>%
  select(matricula, disciplina, media) %>%
  mutate(disciplina = as.factor(gsub(" ", ".", disciplina))) %>%
  dcast(matricula ~disciplina, mean) %>%
  merge(graduados.cra)

#selecionando cra e disciplinas do primeiro e segundo período
p1.p2.train <- graduados.model.input %>%
  select(Laboratório.de.Programação.I, Programação.I, Introdução.à.Computação, Cálculo.Diferencial.e.Integral.I, Álgebra.Vetorial.e.Geometria.Analítica, Leitura.e.Produção.de.Textos, Cálculo.Diferencial.e.Integral.II, Matemática.Discreta, Programação.II, Teoria.dos.Grafos, Fundamentos.de.Física.Clássica, Laboratório.de.Programação.II, cra)

#renomeando colunas
colnames(p1.p2.train) <- c("C1", "Vetorial", "LPT", "P1", "IC", "LP1","C2", "Discreta", "P2", "Grafos", "Física", "LP2", "CRA")



# DADOS TESTE

#ordenando os dados pela matrícula
graduados.test <- graduados.test %>%
  arrange(matricula)

#filtrando dados e removendo os NaNs
graduados.clean <- graduados.test %>%
  filter(!is.na(media))

#calculando CRA dos alunos e salvando numa coluna
graduados.cra <- graduados.clean %>%
  group_by(matricula) %>%
  mutate(cra.contrb = media*creditos) %>%
  summarise(cra = sum(cra.contrb)/sum(creditos))

#utilizando a função dcast para deixar o dataset na forma ideal para a análise
graduados.model.input <- graduados.clean %>%
  group_by(matricula, disciplina) %>%
  filter(media == max(media))%>%
  ungroup() %>%
  select(matricula, disciplina, media) %>%
  mutate(disciplina = as.factor(gsub(" ", ".", disciplina))) %>%
  dcast(matricula ~disciplina, mean) %>%
  merge(graduados.cra)

#selecionando cra e disciplinas do primeiro e segundo período
p1.p2.test <- graduados.model.input %>%
  select(Laboratório.de.Programação.I, Programação.I, Introdução.à.Computação, Cálculo.Diferencial.e.Integral.I, Álgebra.Vetorial.e.Geometria.Analítica, Leitura.e.Produção.de.Textos, Cálculo.Diferencial.e.Integral.II, Matemática.Discreta, Programação.II, Teoria.dos.Grafos, Fundamentos.de.Física.Clássica, Laboratório.de.Programação.II, cra)

#renomeando colunas
colnames(p1.p2.test) <- c("C1", "Vetorial", "LPT", "P1", "IC", "LP1","C2", "Discreta", "P2", "Grafos", "Física", "LP2", "CRA")



#substituindo NaN pelo CRA
for (i in 1:nrow(p1.p2.train)){
  for (j in 1:ncol(p1.p2.train)){
    if(is.na(p1.p2.train[i,j])){
      p1.p2.train[i,j] = p1.p2.train$CRA[i]

    }
    if(is.na(p1.p2.test[i,j])){
      p1.p2.test[i, j] = p1.p2.test$CRA[i]
    }
  }
}

```



O que fazer:
1. Separe os dados em treino e teste (por exemplo 90% para treino 10% para teste).
2. Usando todas as variáveis disponíveis (disciplinas do primeiro e segundo período), use validação cruzada (nos dados de treino) para tunar um modelo de regressão Ridge.
3. Mesmo que item acima mas usando um modelo de regressão Lasso.
4. Compare os dois modelos nos dados de teste em termos de RMSE.
5. Quais as variáveis mais importantes segundo o modelo de regressão Lasso? Alguma variável foi descartada? Quais?
6. Re-treine o melhor modelo (dessa vez nos dados de treino sem validação cruzada) e reporte o RMSE no teste.
7. Use o modelo treinado em 6 e aplique nos dados de teste que vamos disponibilizar.
8. Crie novos atributos a partir dos existentes para tentar melhorar o seu modelo.


##2 Ridge
```{r}
#Modelo Ridge
set.seed(825)

ctrl <- trainControl(method = "repeatedcv", repeats = 5, number = 10)
lambda.grid <- expand.grid(lambda = 10^seq(10, -2, length=100))

ridge <- train(CRA ~ . ,data = p1.p2.train,
                        method = "ridge",
                        tuneGrid = lambda.grid,
                        trControl = ctrl,
                        metric = 'RMSE',
                        preProcess=c('scale', 'center'))
```

A função train utilizou um vetor com 100 possíveis valores para lambda e achou o valor ótimo (RMSE mais baixo).

```{r}
ridge
```

Depois de criado o modelo iremos agora gerar a previsão.

```{r}
ridge_prediction <- predict(ridge, p1.p2.train)

ridge_prediction <- data.frame(pred = ridge_prediction, obs = p1.p2.train$CRA)
round(defaultSummary(ridge_prediction), digits = 3)
```

##3 Lasso
Iremos agora criar um modelo utilizando o Lasso. O Lasso é uma técnica que, além de controlar o overfitting, aplica a seleção de variáveis que melhor explicam a variável resposta. Para criar esse modelo usaremos a biblioteca caret.
```{r}
#Modelo Lasso
set.seed(825)
lasso <- train(CRA ~ . ,data = p1.p2.train,
                        method = "lasso",
                        tuneLength = 10,
                        metric = 'RMSE',
                        preProcess=c('scale', 'center'))
plot(lasso)
```
A função train tentou 10 valores para fraction e achou o valor ótimo (RMSE mais baixo).

```{r}
lasso
```

Depois de criado o modelo iremos agora gerar a previsão.

```{r}
lasso_prediction <- predict(lasso, p1.p2.train)

lasso_prediction <- data.frame(pred = lasso_prediction, obs = p1.p2.train$CRA)
round(defaultSummary(lasso_prediction), digits = 3)

```


##4 Comparando os modelos

```{r}
compare <- ridge_prediction
compare$model <- "RIDGE"

lasso_prediction$model <- "LASSO"

compare <- rbind(compare, lasso_prediction)

ggplot(compare, aes(x = pred, y = obs)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.2)) + 
  facet_grid(. ~ model) + 
  geom_abline() +
  ggtitle("Observado x Previsão (validação)")

```

```{r}
round(defaultSummary(ridge_prediction), digits = 3)
round(defaultSummary(lasso_prediction), digits = 3)
```
O melhor modelo será aquele que possuir o RMSE mais baixo. Como podemos ver, ambos os modelos produziram um RMSE próximo, ou seja, não temos uma diferença tão significativa entre os modelos. Entre o Lasso e o Ridge ficamos com o Ridge que ficou com RMSE ~0.459


##5 Importância das Variáveis

```{r}
plot(varImp(ridge, scale = FALSE))
plot(varImp(lasso, scale = FALSE))
```

```{r}
ggplot(data = p1.p2.train, aes(x = CRA, y = C2)) +
  geom_point()
```
Vamos observar um pouco mais a fundo a relação entre C2 e o CRA, já que os nossos modelos apontaram essa variável como sendo a mais importante. Vamos que C2 e CRA tem um comportamento bem linear, temos alguns outliers, mas podemos concluir que quanto mais alta a nota em C2 consequentemente mais alto o CRA.


##6 Re-treino Ridge
```{r}

r.ridge <- train(CRA ~ . ,data = p1.p2.train,
                        method = "ridge",
                        tuneGrid = lambda.grid,
                        trControl = ctrl,
                        metric = 'RMSE',
                        preProcess=c('scale', 'center'))
r.ridge 
plot(r.lasso)
```

```{r}
#predicao do re-treino
r.ridge_prediction <- predict(r.ridge, p1.p2.test)

r.ridge_prediction <- data.frame(pred = r.ridge_prediction, obs = p1.p2.test$CRA)
round(defaultSummary(r.ridge_prediction), digits = 3)
```

##7 Treino com dados de teste

##8 Comparando Modelos com treino e teste

Para termos uma comparação mais robusta, propusemos um modelo de regressão linear. Primeiro rodamos com todas as variáveis, depois só com as mais significativas e tivemos o seguinte resultado.  
```{r}
lm <- train(CRA ~ . ,p1.p2.train, method= "lm", metric="RMSE") 
summary(lm)

lmFit <- train(CRA ~ ., p1.p2.train %>% select(-C1, -LP2), method= "lm", metric="RMSE") 
summary(lmFit)

lmFit_prediction <- predict(lmFit, p1.p2.train)

lmFit_prediction <- data.frame(pred = lmFit_prediction, obs = p1.p2.train$CRA)
round(defaultSummary(lmFit_prediction), digits = 3)

```
Nosso modelo linear apresentou um RMSE parecido com o dos modelos já expostos, porém com um alto R-squared. No gráfico abaixo podemos analisar nossa comparação.
```{r}

linear_rmse <- round(defaultSummary(lmFit_prediction), digits = 3)[1]
ridge_rmse <- round(defaultSummary(ridge_prediction), digits = 3)[1]
lasso_rmse <- round(defaultSummary(lasso_prediction), digits = 3)[1]
toPlot <- data.frame(RMSE = c(linear_rmse, ridge_rmse, lasso_rmse), 
                     Modelo = c("Linear Regression", "Ridge", "Lasso"))


ggplot(toPlot, aes(x=reorder(Modelo, -RMSE), y=RMSE)) + 
  geom_bar(stat="identity") + 
  labs(x='Modelo', y='RMSE') +
   theme(axis.text.x = element_text(angle = 45, hjust = 1), panel.background=element_blank()) +
  coord_flip() + 
  geom_text(aes(label = RMSE))
```

O melhor modelo será aquele que possuir o RMSE mais baixo. Utilizando essa métrica o melhor modelo gerado foi o utilizando Ridge com o cross validation seguido pelo modelo utilizando regressão linear e depois o Lasso. Vale salientar que a diferença entre os modelos é baixissíma.


##9 Melhorando o modelo

##10 Conclusões
